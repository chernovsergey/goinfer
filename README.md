# goinfer
Inference service 

# TODO
 - Implement correct model parsing
 - Implement correct probability prediction
 - Test performance and ensure 100% CPU usage

# Maybe
 - latency monitoring
 - logging
 - dockerization
